# Web Audio API

- use audio sources
- add effects
- visualizations

## Concepts

### Node
  - the basic building block
  - input, destination, or effect
  - `connect` the output of one node to the inputs of others

### Context
  - contains many Nodes
  - connect Nodes
  - execute processing and decoding

#### Minimal Context

Inputs -> Effects -> Destination

### Audio Source

#### Kinds

- generated by javascript
- decoded from pulse code modulation files
- HTML media elements (`<video>` or `<audio>`)
- WebRTC MediaStream (webcam or microphone)

### Audio Buffer

- channel count set at construction
- each sample is 32 bit float
- each frame has 1-channel samples
- `.length` is number of frames, not samples

```js
const context = new AudioContext();
// 44.1khz sample rate is commonly used
const buffer = context.createBuffer(2, 22050, 44100);
```

#### Planar Vs Interleaved Buffers

##### Planar

`LLLLLLLLRRRRRRRR`

- Used for processing

##### Interleaved

`LRLRLRLRLRLRLRLR`

- Used for playback and storage

#### Audio Channels

- Mono
  - 0
- Stereo
  - 0/L
  - 1/R
- Quad
  - 0/L
  - 1/R
  - 2/Surround Left
  - 3/Surround Right
- 5.1
  - 0/L
  - 1/R
  - 2/Center
  - 3/Low Frequency Effects
  - 4/Surround Left
  - 5/Surround Right

### Up and Down Mixing

When connected inputs and outputs don't have the same channel layout, mixing must occur to resolve the difference.

In the WebAudio API, this mixing follows preset default rules, but can be controlled via `AudioNode.channelInterpretation`.

### Visualizations

For convenient access to data from an `AudioContext`, there is a provided `AnalyserNode` which doesn't alter signals passing thru it.

### Spatialisations


### Fan In and Out

- `ChannelMergerNode`
  - Merge multiple mono inputs to a single multi-channel output
- `ChannelSplitterNode`
  - Split a single multi-channel input to multiple mono outputs